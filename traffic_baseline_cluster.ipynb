{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 생성기가 항상 일정한 값을 출력하게 하기 위해 seed 고정\n",
    "random_seed = 2021\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/USER/week2/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200101</td>\n",
       "      <td>0</td>\n",
       "      <td>83247.0</td>\n",
       "      <td>19128.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>5161.0</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>892.0</td>\n",
       "      <td>32263.0</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>3482.0</td>\n",
       "      <td>11299.0</td>\n",
       "      <td>7072.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>3920.0</td>\n",
       "      <td>2133.0</td>\n",
       "      <td>3799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200101</td>\n",
       "      <td>1</td>\n",
       "      <td>89309.0</td>\n",
       "      <td>19027.0</td>\n",
       "      <td>3337.0</td>\n",
       "      <td>5502.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>35609.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>3849.0</td>\n",
       "      <td>13180.0</td>\n",
       "      <td>8771.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>3763.0</td>\n",
       "      <td>642.333333</td>\n",
       "      <td>3483.0</td>\n",
       "      <td>2057.0</td>\n",
       "      <td>4010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200101</td>\n",
       "      <td>2</td>\n",
       "      <td>66611.0</td>\n",
       "      <td>14710.0</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>4631.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>26821.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>...</td>\n",
       "      <td>768.0</td>\n",
       "      <td>2299.0</td>\n",
       "      <td>7986.0</td>\n",
       "      <td>5426.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>3229.0</td>\n",
       "      <td>536.666667</td>\n",
       "      <td>2634.0</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>3388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200101</td>\n",
       "      <td>3</td>\n",
       "      <td>53290.0</td>\n",
       "      <td>13753.0</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>21322.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>...</td>\n",
       "      <td>632.0</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>5703.0</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>2882.0</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>2488.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>3686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200101</td>\n",
       "      <td>4</td>\n",
       "      <td>52095.0</td>\n",
       "      <td>17615.0</td>\n",
       "      <td>2406.0</td>\n",
       "      <td>3689.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>1080.5</td>\n",
       "      <td>22711.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>...</td>\n",
       "      <td>875.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>5816.0</td>\n",
       "      <td>2933.0</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2433.0</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>2952.0</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>5608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>20200517</td>\n",
       "      <td>19</td>\n",
       "      <td>311727.0</td>\n",
       "      <td>101285.0</td>\n",
       "      <td>10085.0</td>\n",
       "      <td>30637.0</td>\n",
       "      <td>10060.0</td>\n",
       "      <td>8749.0</td>\n",
       "      <td>148935.0</td>\n",
       "      <td>6801.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6726.0</td>\n",
       "      <td>15431.0</td>\n",
       "      <td>25597.0</td>\n",
       "      <td>14292.0</td>\n",
       "      <td>9300.0</td>\n",
       "      <td>22238.0</td>\n",
       "      <td>3786.000000</td>\n",
       "      <td>16936.0</td>\n",
       "      <td>10729.0</td>\n",
       "      <td>20194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>20200517</td>\n",
       "      <td>20</td>\n",
       "      <td>305354.0</td>\n",
       "      <td>91426.0</td>\n",
       "      <td>8607.0</td>\n",
       "      <td>26021.0</td>\n",
       "      <td>8095.0</td>\n",
       "      <td>7198.0</td>\n",
       "      <td>136503.0</td>\n",
       "      <td>6147.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5501.0</td>\n",
       "      <td>15378.0</td>\n",
       "      <td>24661.0</td>\n",
       "      <td>14747.0</td>\n",
       "      <td>8239.0</td>\n",
       "      <td>20604.0</td>\n",
       "      <td>3203.000000</td>\n",
       "      <td>15018.0</td>\n",
       "      <td>9767.0</td>\n",
       "      <td>17962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>20200517</td>\n",
       "      <td>21</td>\n",
       "      <td>306008.0</td>\n",
       "      <td>75113.0</td>\n",
       "      <td>6325.0</td>\n",
       "      <td>19933.0</td>\n",
       "      <td>5711.0</td>\n",
       "      <td>4494.0</td>\n",
       "      <td>129412.0</td>\n",
       "      <td>5134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4216.0</td>\n",
       "      <td>12558.0</td>\n",
       "      <td>22781.0</td>\n",
       "      <td>14081.0</td>\n",
       "      <td>6392.0</td>\n",
       "      <td>17937.0</td>\n",
       "      <td>2447.000000</td>\n",
       "      <td>12403.0</td>\n",
       "      <td>7825.0</td>\n",
       "      <td>14031.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>20200517</td>\n",
       "      <td>22</td>\n",
       "      <td>237447.0</td>\n",
       "      <td>49498.0</td>\n",
       "      <td>4209.0</td>\n",
       "      <td>12145.0</td>\n",
       "      <td>3891.0</td>\n",
       "      <td>2718.0</td>\n",
       "      <td>96698.0</td>\n",
       "      <td>3526.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2578.0</td>\n",
       "      <td>8870.0</td>\n",
       "      <td>16640.0</td>\n",
       "      <td>11066.0</td>\n",
       "      <td>4427.0</td>\n",
       "      <td>11955.0</td>\n",
       "      <td>1495.000000</td>\n",
       "      <td>7507.0</td>\n",
       "      <td>5387.0</td>\n",
       "      <td>8889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>20200517</td>\n",
       "      <td>23</td>\n",
       "      <td>150312.0</td>\n",
       "      <td>27410.0</td>\n",
       "      <td>2350.0</td>\n",
       "      <td>6406.0</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>55788.0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1377.0</td>\n",
       "      <td>5021.0</td>\n",
       "      <td>10058.0</td>\n",
       "      <td>7139.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>6844.0</td>\n",
       "      <td>1495.000000</td>\n",
       "      <td>4116.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>4606.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            날짜  시간        10       100      101      120      121     140  \\\n",
       "0     20200101   0   83247.0   19128.0   2611.0   5161.0   1588.0   892.0   \n",
       "1     20200101   1   89309.0   19027.0   3337.0   5502.0   1650.0  1043.0   \n",
       "2     20200101   2   66611.0   14710.0   2970.0   4631.0   1044.0   921.0   \n",
       "3     20200101   3   53290.0   13753.0   2270.0   4242.0   1021.0   790.0   \n",
       "4     20200101   4   52095.0   17615.0   2406.0   3689.0   1840.0  1080.5   \n",
       "...        ...  ..       ...       ...      ...      ...      ...     ...   \n",
       "3307  20200517  19  311727.0  101285.0  10085.0  30637.0  10060.0  8749.0   \n",
       "3308  20200517  20  305354.0   91426.0   8607.0  26021.0   8095.0  7198.0   \n",
       "3309  20200517  21  306008.0   75113.0   6325.0  19933.0   5711.0  4494.0   \n",
       "3310  20200517  22  237447.0   49498.0   4209.0  12145.0   3891.0  2718.0   \n",
       "3311  20200517  23  150312.0   27410.0   2350.0   6406.0   1803.0  1614.0   \n",
       "\n",
       "           150     160  ...    1020     1040     1100     1200    1510  \\\n",
       "0      32263.0  1636.0  ...  1311.0   3482.0  11299.0   7072.0  1176.0   \n",
       "1      35609.0  1644.0  ...  1162.0   3849.0  13180.0   8771.0  1283.0   \n",
       "2      26821.0  1104.0  ...   768.0   2299.0   7986.0   5426.0  1536.0   \n",
       "3      21322.0   909.0  ...   632.0   1716.0   5703.0   3156.0  1104.0   \n",
       "4      22711.0  1354.0  ...   875.0   2421.0   5816.0   2933.0  1206.0   \n",
       "...        ...     ...  ...     ...      ...      ...      ...     ...   \n",
       "3307  148935.0  6801.0  ...  6726.0  15431.0  25597.0  14292.0  9300.0   \n",
       "3308  136503.0  6147.0  ...  5501.0  15378.0  24661.0  14747.0  8239.0   \n",
       "3309  129412.0  5134.0  ...  4216.0  12558.0  22781.0  14081.0  6392.0   \n",
       "3310   96698.0  3526.0  ...  2578.0   8870.0  16640.0  11066.0  4427.0   \n",
       "3311   55788.0  1849.0  ...  1377.0   5021.0  10058.0   7139.0  2250.0   \n",
       "\n",
       "         2510         3000     4510     5510     6000  \n",
       "0      3810.0   748.000000   3920.0   2133.0   3799.0  \n",
       "1      3763.0   642.333333   3483.0   2057.0   4010.0  \n",
       "2      3229.0   536.666667   2634.0   1526.0   3388.0  \n",
       "3      2882.0   431.000000   2488.0   1268.0   3686.0  \n",
       "4      2433.0   499.000000   2952.0   1927.0   5608.0  \n",
       "...       ...          ...      ...      ...      ...  \n",
       "3307  22238.0  3786.000000  16936.0  10729.0  20194.0  \n",
       "3308  20604.0  3203.000000  15018.0   9767.0  17962.0  \n",
       "3309  17937.0  2447.000000  12403.0   7825.0  14031.0  \n",
       "3310  11955.0  1495.000000   7507.0   5387.0   8889.0  \n",
       "3311   6844.0  1495.000000   4116.0   3046.0   4606.0  \n",
       "\n",
       "[3312 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'train_edit.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val=pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200511</td>\n",
       "      <td>0</td>\n",
       "      <td>77968</td>\n",
       "      <td>14429</td>\n",
       "      <td>1233</td>\n",
       "      <td>4021</td>\n",
       "      <td>981</td>\n",
       "      <td>881</td>\n",
       "      <td>28672</td>\n",
       "      <td>1064</td>\n",
       "      <td>...</td>\n",
       "      <td>637</td>\n",
       "      <td>2604</td>\n",
       "      <td>5239</td>\n",
       "      <td>4168</td>\n",
       "      <td>1155</td>\n",
       "      <td>3596</td>\n",
       "      <td>337</td>\n",
       "      <td>2262</td>\n",
       "      <td>1608</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200511</td>\n",
       "      <td>1</td>\n",
       "      <td>48679</td>\n",
       "      <td>9136</td>\n",
       "      <td>823</td>\n",
       "      <td>2618</td>\n",
       "      <td>654</td>\n",
       "      <td>572</td>\n",
       "      <td>17722</td>\n",
       "      <td>672</td>\n",
       "      <td>...</td>\n",
       "      <td>353</td>\n",
       "      <td>1870</td>\n",
       "      <td>3359</td>\n",
       "      <td>2558</td>\n",
       "      <td>1002</td>\n",
       "      <td>2157</td>\n",
       "      <td>257</td>\n",
       "      <td>1425</td>\n",
       "      <td>1018</td>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200511</td>\n",
       "      <td>2</td>\n",
       "      <td>33773</td>\n",
       "      <td>8199</td>\n",
       "      <td>578</td>\n",
       "      <td>2188</td>\n",
       "      <td>392</td>\n",
       "      <td>502</td>\n",
       "      <td>14464</td>\n",
       "      <td>579</td>\n",
       "      <td>...</td>\n",
       "      <td>345</td>\n",
       "      <td>1499</td>\n",
       "      <td>2646</td>\n",
       "      <td>2022</td>\n",
       "      <td>876</td>\n",
       "      <td>1959</td>\n",
       "      <td>232</td>\n",
       "      <td>1155</td>\n",
       "      <td>927</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200511</td>\n",
       "      <td>3</td>\n",
       "      <td>41511</td>\n",
       "      <td>9986</td>\n",
       "      <td>726</td>\n",
       "      <td>2817</td>\n",
       "      <td>555</td>\n",
       "      <td>646</td>\n",
       "      <td>17793</td>\n",
       "      <td>650</td>\n",
       "      <td>...</td>\n",
       "      <td>390</td>\n",
       "      <td>1730</td>\n",
       "      <td>3398</td>\n",
       "      <td>1967</td>\n",
       "      <td>912</td>\n",
       "      <td>2462</td>\n",
       "      <td>281</td>\n",
       "      <td>1477</td>\n",
       "      <td>959</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200511</td>\n",
       "      <td>4</td>\n",
       "      <td>78680</td>\n",
       "      <td>19509</td>\n",
       "      <td>1463</td>\n",
       "      <td>4720</td>\n",
       "      <td>825</td>\n",
       "      <td>1088</td>\n",
       "      <td>35125</td>\n",
       "      <td>997</td>\n",
       "      <td>...</td>\n",
       "      <td>679</td>\n",
       "      <td>2958</td>\n",
       "      <td>7369</td>\n",
       "      <td>4120</td>\n",
       "      <td>1569</td>\n",
       "      <td>4568</td>\n",
       "      <td>577</td>\n",
       "      <td>3155</td>\n",
       "      <td>1871</td>\n",
       "      <td>3656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20200524</td>\n",
       "      <td>19</td>\n",
       "      <td>314226</td>\n",
       "      <td>98345</td>\n",
       "      <td>10625</td>\n",
       "      <td>28618</td>\n",
       "      <td>8316</td>\n",
       "      <td>6684</td>\n",
       "      <td>141675</td>\n",
       "      <td>6619</td>\n",
       "      <td>...</td>\n",
       "      <td>8254</td>\n",
       "      <td>16118</td>\n",
       "      <td>23304</td>\n",
       "      <td>14082</td>\n",
       "      <td>8447</td>\n",
       "      <td>21694</td>\n",
       "      <td>2180</td>\n",
       "      <td>15746</td>\n",
       "      <td>10903</td>\n",
       "      <td>21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20200524</td>\n",
       "      <td>20</td>\n",
       "      <td>300001</td>\n",
       "      <td>87871</td>\n",
       "      <td>8226</td>\n",
       "      <td>22706</td>\n",
       "      <td>6981</td>\n",
       "      <td>5743</td>\n",
       "      <td>142933</td>\n",
       "      <td>6295</td>\n",
       "      <td>...</td>\n",
       "      <td>5225</td>\n",
       "      <td>15297</td>\n",
       "      <td>21919</td>\n",
       "      <td>14526</td>\n",
       "      <td>7332</td>\n",
       "      <td>19732</td>\n",
       "      <td>1990</td>\n",
       "      <td>14096</td>\n",
       "      <td>10028</td>\n",
       "      <td>17787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20200524</td>\n",
       "      <td>21</td>\n",
       "      <td>304150</td>\n",
       "      <td>71126</td>\n",
       "      <td>6002</td>\n",
       "      <td>18317</td>\n",
       "      <td>4939</td>\n",
       "      <td>3779</td>\n",
       "      <td>133110</td>\n",
       "      <td>4781</td>\n",
       "      <td>...</td>\n",
       "      <td>4072</td>\n",
       "      <td>12685</td>\n",
       "      <td>21135</td>\n",
       "      <td>14403</td>\n",
       "      <td>5443</td>\n",
       "      <td>16967</td>\n",
       "      <td>1359</td>\n",
       "      <td>11670</td>\n",
       "      <td>7963</td>\n",
       "      <td>14041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20200524</td>\n",
       "      <td>22</td>\n",
       "      <td>236751</td>\n",
       "      <td>44947</td>\n",
       "      <td>3575</td>\n",
       "      <td>11455</td>\n",
       "      <td>3135</td>\n",
       "      <td>2536</td>\n",
       "      <td>98582</td>\n",
       "      <td>3267</td>\n",
       "      <td>...</td>\n",
       "      <td>2489</td>\n",
       "      <td>8093</td>\n",
       "      <td>14427</td>\n",
       "      <td>10914</td>\n",
       "      <td>3861</td>\n",
       "      <td>11397</td>\n",
       "      <td>859</td>\n",
       "      <td>7270</td>\n",
       "      <td>5194</td>\n",
       "      <td>8230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20200524</td>\n",
       "      <td>23</td>\n",
       "      <td>143609</td>\n",
       "      <td>26137</td>\n",
       "      <td>2242</td>\n",
       "      <td>6166</td>\n",
       "      <td>1609</td>\n",
       "      <td>1391</td>\n",
       "      <td>54633</td>\n",
       "      <td>1899</td>\n",
       "      <td>...</td>\n",
       "      <td>1343</td>\n",
       "      <td>4686</td>\n",
       "      <td>8732</td>\n",
       "      <td>6986</td>\n",
       "      <td>2161</td>\n",
       "      <td>6487</td>\n",
       "      <td>410</td>\n",
       "      <td>3963</td>\n",
       "      <td>2686</td>\n",
       "      <td>4690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜  시간      10    100    101    120   121   140     150   160  ...  \\\n",
       "0    20200511   0   77968  14429   1233   4021   981   881   28672  1064  ...   \n",
       "1    20200511   1   48679   9136    823   2618   654   572   17722   672  ...   \n",
       "2    20200511   2   33773   8199    578   2188   392   502   14464   579  ...   \n",
       "3    20200511   3   41511   9986    726   2817   555   646   17793   650  ...   \n",
       "4    20200511   4   78680  19509   1463   4720   825  1088   35125   997  ...   \n",
       "..        ...  ..     ...    ...    ...    ...   ...   ...     ...   ...  ...   \n",
       "331  20200524  19  314226  98345  10625  28618  8316  6684  141675  6619  ...   \n",
       "332  20200524  20  300001  87871   8226  22706  6981  5743  142933  6295  ...   \n",
       "333  20200524  21  304150  71126   6002  18317  4939  3779  133110  4781  ...   \n",
       "334  20200524  22  236751  44947   3575  11455  3135  2536   98582  3267  ...   \n",
       "335  20200524  23  143609  26137   2242   6166  1609  1391   54633  1899  ...   \n",
       "\n",
       "     1020   1040   1100   1200  1510   2510  3000   4510   5510   6000  \n",
       "0     637   2604   5239   4168  1155   3596   337   2262   1608   2337  \n",
       "1     353   1870   3359   2558  1002   2157   257   1425   1018   1810  \n",
       "2     345   1499   2646   2022   876   1959   232   1155    927   1530  \n",
       "3     390   1730   3398   1967   912   2462   281   1477    959   1882  \n",
       "4     679   2958   7369   4120  1569   4568   577   3155   1871   3656  \n",
       "..    ...    ...    ...    ...   ...    ...   ...    ...    ...    ...  \n",
       "331  8254  16118  23304  14082  8447  21694  2180  15746  10903  21014  \n",
       "332  5225  15297  21919  14526  7332  19732  1990  14096  10028  17787  \n",
       "333  4072  12685  21135  14403  5443  16967  1359  11670   7963  14041  \n",
       "334  2489   8093  14427  10914  3861  11397   859   7270   5194   8230  \n",
       "335  1343   4686   8732   6986  2161   6487   410   3963   2686   4690  \n",
       "\n",
       "[336 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200518</td>\n",
       "      <td>0</td>\n",
       "      <td>82065</td>\n",
       "      <td>15172</td>\n",
       "      <td>1500</td>\n",
       "      <td>3294</td>\n",
       "      <td>1086</td>\n",
       "      <td>962</td>\n",
       "      <td>28931</td>\n",
       "      <td>1103</td>\n",
       "      <td>...</td>\n",
       "      <td>618</td>\n",
       "      <td>2790</td>\n",
       "      <td>5147</td>\n",
       "      <td>4331</td>\n",
       "      <td>1329</td>\n",
       "      <td>3665</td>\n",
       "      <td>404</td>\n",
       "      <td>2242</td>\n",
       "      <td>1619</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200518</td>\n",
       "      <td>1</td>\n",
       "      <td>51248</td>\n",
       "      <td>9840</td>\n",
       "      <td>813</td>\n",
       "      <td>2356</td>\n",
       "      <td>696</td>\n",
       "      <td>546</td>\n",
       "      <td>17888</td>\n",
       "      <td>720</td>\n",
       "      <td>...</td>\n",
       "      <td>430</td>\n",
       "      <td>1864</td>\n",
       "      <td>3269</td>\n",
       "      <td>2561</td>\n",
       "      <td>921</td>\n",
       "      <td>2081</td>\n",
       "      <td>272</td>\n",
       "      <td>1390</td>\n",
       "      <td>1003</td>\n",
       "      <td>1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200518</td>\n",
       "      <td>2</td>\n",
       "      <td>39026</td>\n",
       "      <td>7894</td>\n",
       "      <td>760</td>\n",
       "      <td>2413</td>\n",
       "      <td>408</td>\n",
       "      <td>549</td>\n",
       "      <td>13357</td>\n",
       "      <td>498</td>\n",
       "      <td>...</td>\n",
       "      <td>322</td>\n",
       "      <td>1313</td>\n",
       "      <td>2765</td>\n",
       "      <td>1931</td>\n",
       "      <td>920</td>\n",
       "      <td>1764</td>\n",
       "      <td>228</td>\n",
       "      <td>1136</td>\n",
       "      <td>922</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200518</td>\n",
       "      <td>3</td>\n",
       "      <td>40993</td>\n",
       "      <td>10137</td>\n",
       "      <td>780</td>\n",
       "      <td>2701</td>\n",
       "      <td>420</td>\n",
       "      <td>741</td>\n",
       "      <td>15544</td>\n",
       "      <td>532</td>\n",
       "      <td>...</td>\n",
       "      <td>326</td>\n",
       "      <td>1766</td>\n",
       "      <td>3320</td>\n",
       "      <td>2060</td>\n",
       "      <td>892</td>\n",
       "      <td>2447</td>\n",
       "      <td>337</td>\n",
       "      <td>1495</td>\n",
       "      <td>975</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200518</td>\n",
       "      <td>4</td>\n",
       "      <td>77863</td>\n",
       "      <td>19603</td>\n",
       "      <td>1276</td>\n",
       "      <td>5019</td>\n",
       "      <td>968</td>\n",
       "      <td>1160</td>\n",
       "      <td>32101</td>\n",
       "      <td>968</td>\n",
       "      <td>...</td>\n",
       "      <td>669</td>\n",
       "      <td>2914</td>\n",
       "      <td>6986</td>\n",
       "      <td>3911</td>\n",
       "      <td>1368</td>\n",
       "      <td>4380</td>\n",
       "      <td>513</td>\n",
       "      <td>2940</td>\n",
       "      <td>1758</td>\n",
       "      <td>3629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>20200531</td>\n",
       "      <td>19</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20200531</td>\n",
       "      <td>20</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20200531</td>\n",
       "      <td>21</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20200531</td>\n",
       "      <td>22</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20200531</td>\n",
       "      <td>23</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>...</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜  시간     10    100   101   120   121   140    150   160  ...  \\\n",
       "0    20200518   0  82065  15172  1500  3294  1086   962  28931  1103  ...   \n",
       "1    20200518   1  51248   9840   813  2356   696   546  17888   720  ...   \n",
       "2    20200518   2  39026   7894   760  2413   408   549  13357   498  ...   \n",
       "3    20200518   3  40993  10137   780  2701   420   741  15544   532  ...   \n",
       "4    20200518   4  77863  19603  1276  5019   968  1160  32101   968  ...   \n",
       "..        ...  ..    ...    ...   ...   ...   ...   ...    ...   ...  ...   \n",
       "331  20200531  19   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "332  20200531  20   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "333  20200531  21   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "334  20200531  22   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "335  20200531  23   -999   -999  -999  -999  -999  -999   -999  -999  ...   \n",
       "\n",
       "     1020  1040  1100  1200  1510  2510  3000  4510  5510  6000  \n",
       "0     618  2790  5147  4331  1329  3665   404  2242  1619  2314  \n",
       "1     430  1864  3269  2561   921  2081   272  1390  1003  1766  \n",
       "2     322  1313  2765  1931   920  1764   228  1136   922  1309  \n",
       "3     326  1766  3320  2060   892  2447   337  1495   975  1912  \n",
       "4     669  2914  6986  3911  1368  4380   513  2940  1758  3629  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "331  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "332  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "333  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "334  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "335  -999  -999  -999  -999  -999  -999  -999  -999  -999  -999  \n",
       "\n",
       "[336 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataloader\n",
    "* 한 칼럼에 대한 7일(168행) 데이터를 input_data, 뒤따르는 7일 데이터를 output_data로 반환합니다.\n",
    "* 도로별 차이를 두지 않고 모든 도로를 동일한 타입의 데이터로 취급합니다.\n",
    "* 모든 csv 파일의 마지막 168행은 예측해야하는 값이므로 input으로 들어가지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = ['150','500','1000']\n",
    "cluster_1 =['100', '251', '352', '450', '550']\n",
    "cluster_2 = ['101', '120', '121', '140', '160', '200', '201', '270', '300', '301',\n",
    "       '351', '370', '400', '600', '650', '652', '1020', '1040', '1100',\n",
    "       '1200', '1510', '2510', '3000', '4510', '5510', '6000']\n",
    "cluster_3 = ['10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클러스터 정보를 추가하기 위해 베이스라인 코드 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):      # torch.utils.data.Dataset 클래스의 상속 클래스 CustomDataset class 생성. 상속 클래스 생성시 __init__, __getitem__, __len__함수는 기본적으로 정의해줘야 함.\n",
    "    \n",
    "    def __init__(self, root, seq_len, cat, batch_size=64, phase='train'):      # 데이터 로드 단계에 사용될 여러 변수들을 'self.변수명'의 형태로 지정해두는 함수\n",
    "        \n",
    "        self.root = root      # CustomDataset 객체 생성 시 데이터 경로 앞부분(공통 부분)을 root로 입력받아 저장\n",
    "        self.phase = phase      # CustomDataset 객체 생성 시 데이터 경로 뒷부분(train/validate/test)을 phase로 입력받아 저장\n",
    "        self.label_path = os.path.join(self.root, self.phase + '.csv')      # 데이터 전체 경로 생성\n",
    "        df = pd.read_csv(self.label_path)      # 생성한 데이터 전체 경로로부터 데이터 로드\n",
    "        \n",
    "        self.seq_len = seq_len * 24      # 일 단위 기간을 입력 받은 후 시간 단위 기간으로 변환하여 저장\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "        self.cat = cat                   \n",
    "        self.highways = len(self.cat)\n",
    "        timestamps = [(i, j) for (i, j) in zip(list(df['날짜']), list(df['시간']))]      # 날짜와 시간 정보가 튜플로 들어 있는 리스트 생성\n",
    "        # categories = df.columns.values.tolist()[2:]      # 도로명 column list 생성\n",
    "\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        for t in range(len(timestamps)):  \n",
    "            temp_input_data = []\n",
    "            temp_output_data = []\n",
    "            for col in self.cat:\n",
    "                road = df[col].tolist()        \n",
    "                inp = [float(i) for i in road[t:t+self.seq_len]]      # input 데이터 시계열 구간 설정\n",
    "                outp = [float(j) for j in road[t+self.seq_len:t+2*self.seq_len]]      # output 데이터 시계열 구간 설정\n",
    "                temp_input_data.append(inp) \n",
    "                temp_output_data.append(outp)\n",
    "            input_data.append(temp_input_data)\n",
    "            output_data.append(temp_output_data)\n",
    "            \n",
    "# input_data : [[첫번째 input 기간 동안의 첫번째 도로의 통행량 list, ..., 첫번째 input 기간 동안의 35번째 도로의 통행량 list], ...,\n",
    "#               [마지막 input 기간 동안의 첫번째 도로의 통행량 list, ..., 마지막 input 기간 동안의 35번째 도로의 통행량 list]]\n",
    "# output_data : [[첫번째 output 기간 동안의 첫번째 도로의 통행량 list, ..., 첫번째 output 기간 동안의 35번째 도로의 통행량 list], ...,\n",
    "#                [마지막 output 기간 동안의 첫번째 도로의 통행량 list, ..., 마지막 output 기간 동안의 35번째 도로의 통행량 list]]\n",
    "        \n",
    "        self.labels['timestamp'] = timestamps\n",
    "        self.labels['category'] = self.cat\n",
    "        self.labels['input'] = input_data\n",
    "        self.labels['output'] = output_data\n",
    "\n",
    "    def __getitem__(self, index):      # index를 가지고 데이터를 하나씩 불러올 수 있게 하는 함수\n",
    "\n",
    "#         데이터 내 index가 부여되는 형태\n",
    "\n",
    "#                 | road_1    road_2    ...  road_35\n",
    "#                -------------------------------------\n",
    "#         time_1  | index_0   index_1   ...  index_34\n",
    "#         time_2  | index_35  index_36  ...  index_69\n",
    "\n",
    "        row = index // self.highways      # index를 35(도로수)로 나눈 몫  ex) 71//35 -> 2\n",
    "        col = index % self.highways      # index를 35(도로수)로 나눈 나머지  ex) 71%35 -> 1\n",
    "\n",
    "        timestamp = self.labels['timestamp'][row]      # (날짜, 시간) 튜플이 들어있는 list에서 row번째 시점에 해당하는 튜플\n",
    "        category = self.labels['category'][col]      # 도로명 column list에서 col번째 도로에 해당하는 element\n",
    "        \n",
    "        input_data = torch.tensor(self.labels['input'][row][col])      # input_data list에서, row번째 시점의 col번째 도로 교통량 정보\n",
    "\n",
    "        if self.phase != 'test':\n",
    "            output_data = torch.tensor(self.labels['output'][row][col])\n",
    "        else:\n",
    "            output_data = []\n",
    "\n",
    "        return timestamp, category, (input_data, output_data)\n",
    "    \n",
    "    def __len__(self):      # getitem 함수를 통해 데이터를 불러오려면,전체 index 길이를 알아야 한다.\n",
    "        return (len(self.labels['timestamp'])-(self.seq_len * 2)+1) * self.highways      # 특정 시점이 아닌 특정 기간을 하나의 data 단위로 설정하면, 전체 샘플 수는 감소함을 반영 \n",
    "\n",
    "\n",
    "def data_loader(root,cat, phase='train', batch_size=64, seq_len=7, drop_last=False):\n",
    "    if phase == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "    cat =  cat\n",
    "    dataset = CustomDataset(root, seq_len, cat, batch_size, phase)\n",
    "    dataloader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=168,      # input 길이는 168시간(7일 X 24시간)\n",
    "                 hidden_size=512,\n",
    "                 output_size=168,      # output 길이는 168시간(7일 X 24시간)\n",
    "                 batch_size=128,\n",
    "                 num_layers=3,\n",
    "                 dropout=0,\n",
    "                 batch_first=False):      # batch_first(default=False) : 배치 차원을 첫번째 차원으로 하여 데이터를 불러올 것인지 여부\n",
    "\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        ##### Layer 1\n",
    "        self.lstm1 = nn.LSTM(input_size,\n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "\n",
    "        #### Layer 2\n",
    "        self.lstm2 = nn.LSTM(hidden_size, \n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "\n",
    "        ##### Finalize\n",
    "        self.linear = nn.Linear(hidden_size, \n",
    "                                output_size)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x, h_in, c_in):\n",
    "\n",
    "        h_in = nn.Parameter(h_in.type(dtype), requires_grad=True)      # gradient descent로 업데이트 되는(requires_grad=True), h_in 이라는 이름의 파라미터 생성 \n",
    "        c_in = nn.Parameter(c_in.type(dtype), requires_grad=True)      # gradient descent로 업데이트 되는(requires_grad=True), c_in 이라는 이름의 파라미터 생성\n",
    "\n",
    "        # Layer 1\n",
    "        lstm_out, (h_1, c_1) = self.lstm1(x, (h_in, c_in))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        #Layer2\n",
    "        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "        # Final\n",
    "        predictions = self.linear(lstm_out)\n",
    "        \n",
    "        return predictions, (h_2, c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일과 모델 가중치 파일 저장을 위해 log 디렉토리 생성. 중요한 파일이 덮어씌워지지 않도록 주의\n",
    "os.makedirs('log', exist_ok=True)\n",
    "\n",
    "\n",
    "def save_model(model_name, model, optimizer):\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, os.path.join('log', model_name + '.pth'))\n",
    "    print('model saved\\n')\n",
    "    return os.path.join('log', model_name + '.pth')\n",
    "\n",
    "\n",
    "def load_model(model_name, model, optimizer=None):\n",
    "    state = torch.load(os.path.join(model_name))\n",
    "    model.load_state_dict(state['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "model_name = 'sequential'\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "val_epoch = 1\n",
    "base_lr = 0.01\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "output_size = input_size\n",
    "hidden_size = 512\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)      # optimizer로는 Adam이 가장 무난합니다. Adam을 쓰면 learning_rate를 따로 지정해주지 않아도 알아서 조정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (lstm1): LSTM(168, 512, num_layers=6, dropout=0.2)\n",
      "  (lstm2): LSTM(512, 512, num_layers=6, dropout=0.2)\n",
      "  (linear): Linear(in_features=512, out_features=168, bias=True)\n",
      "  (activation): LeakyReLU(negative_slope=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get data loader\n",
    "train_dataloader = data_loader(root=DATASET_PATH,\n",
    "                               cat= cluster_3,\n",
    "                               phase='train_edit',\n",
    "                               batch_size=batch_size,\n",
    "                               seq_len=seq_len,\n",
    "                               drop_last=True)\n",
    "\n",
    "validate_dataloader = data_loader(root=DATASET_PATH,\n",
    "                                  cat= cluster_3,\n",
    "                                  phase='validate',\n",
    "                                  batch_size=1,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CustomDataset( DATASET_PATH, seq_len,cat=cluster_3, batch_size=64, phase='train')\n",
    "b = CustomDataset( DATASET_PATH, seq_len,cat=cluster_3, batch_size=64, phase='validate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20200101, 0),\n",
       " '10',\n",
       " (tensor([ 83247.,  89309.,  66611.,  53290.,  52095.,  62591.,  72799.,  98367.,\n",
       "          138272., 204720., 285893., 324836., 320617., 331264., 359475., 384305.,\n",
       "          387721., 354546., 299045., 272186., 256661., 230300., 170933., 107623.,\n",
       "           62778.,  40492.,  32815.,  36717.,  64090., 142571., 248187., 321245.,\n",
       "          302631., 282018., 307976., 324038., 321212., 331174., 337396., 340639.,\n",
       "          338451., 333273., 298945., 241560., 217665., 194461., 157652., 112308.,\n",
       "           82799.,  59306.,  48045.,  49712.,  67423., 115647., 194381., 280641.,\n",
       "          299510., 300402., 320228., 332667., 333576., 340362., 366144., 386427.,\n",
       "          400061., 421336., 407648., 345687., 306051., 268332., 208612., 144902.,\n",
       "           97966.,  69127.,  52807.,  47297.,  56987.,  90089., 144122., 199788.,\n",
       "          268007., 338748., 385538., 387355., 356511., 363416., 388014., 399648.,\n",
       "          401181., 378500., 321310., 259903., 232601., 210189., 163773., 113008.,\n",
       "           71491.,  47490.,  32479.,  26481.,  27540.,  34262.,  69992., 107845.,\n",
       "          157837., 225944., 293550., 325889., 325776., 344459., 384079., 404593.,\n",
       "          412639., 382811., 334494., 296820., 278469., 262718., 201091., 131401.,\n",
       "           78735.,  51570.,  40050.,  42615.,  72065., 160194., 281763., 342426.,\n",
       "          324735., 301869., 320709., 321756., 305470., 310225., 312814., 321922.,\n",
       "          321846., 324081., 296784., 235546., 210770., 184579., 149213., 109713.,\n",
       "           79392.,  55685.,  44636.,  45419.,  56972.,  93447., 171618., 264769.,\n",
       "          287526., 275884., 278635., 268676., 282590., 298843., 310272., 312283.,\n",
       "          320667., 311848., 291672., 234126., 193417., 178524., 144937., 109482.]),\n",
       "  tensor([ 76476.,  55529.,  44368.,  44271.,  58938.,  99831., 178747., 293117.,\n",
       "          303530., 300989., 316233., 319801., 310324., 320176., 326347., 352508.,\n",
       "          361507., 353063., 313991., 252862., 216526., 195100., 163267., 119917.,\n",
       "           85400.,  62119.,  48952.,  51537.,  68192., 121927., 212732., 310369.,\n",
       "          337781., 340532., 331328., 334324., 329683., 336410., 357612., 355231.,\n",
       "          374556., 379997., 333143., 269262., 239292., 211268., 154358., 124997.,\n",
       "           87880.,  63484.,  50226.,  51796.,  69432., 114223., 206106., 301482.,\n",
       "          329924., 337799., 346933., 352988., 351011., 373930., 381683., 403499.,\n",
       "          428829., 444463., 427831., 370201., 331317., 286966., 219289., 154247.,\n",
       "          113399.,  75694.,  56122.,  50606.,  58192.,  94993., 153701., 225538.,\n",
       "          291212., 350132., 399522., 403347., 371253., 381428., 401492., 418788.,\n",
       "          425706., 405086., 347134., 282694., 248778., 228066., 181447., 123481.,\n",
       "           81601.,  52624.,  37084.,  28862.,  29909.,  46505.,  80107., 124912.,\n",
       "          189697., 271575., 343397., 367597., 363550., 386975., 421540., 448157.,\n",
       "          471607., 438078., 378585., 336731., 317128., 297730., 227666., 148221.,\n",
       "           90257.,  54192.,  44037.,  47253.,  81531., 181714., 310599., 389790.,\n",
       "          373126., 356902., 377961., 371933., 357853., 366356., 371590., 372055.,\n",
       "          392235., 393977., 349005., 275467., 241945., 217081., 176465., 127390.,\n",
       "           89802.,  67210.,  54055.,  55418.,  71595., 125896., 225517., 327896.,\n",
       "          393354., 375529., 312747., 370539., 361755., 367599., 383257., 397238.,\n",
       "          416391., 414138., 364889., 290906., 252207., 220641., 177148., 131423.])))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20200518, 3),\n",
       " '352',\n",
       " (tensor([ 6202., 11987., 34498., 58310., 74453., 68887., 65707., 66946., 66569.,\n",
       "          63158., 60856., 66447., 66419., 61933., 58214., 59437., 41850., 41873.,\n",
       "          29095., 19299., 13873., 10253.,  6710.,  6184.,  6578.,  9001., 20300.,\n",
       "          36178., 52825., 59803., 61520., 59795., 61635., 57030., 59370., 61211.,\n",
       "          65022., 66107., 63431., 56580., 41155., 32446., 27445., 20854., 15498.,\n",
       "          10810.,  7677.,  6931.,  7124., 11882., 28467., 48141., 61618., 64950.,\n",
       "          69321., 72998., 70947., 68448., 68544., 69960., 71414., 74861., 73614.,\n",
       "          69255., 50034., 39784., 31584., 23496., 16309., 10679.,  7707.,  6968.,\n",
       "           7049., 11152., 27273., 44909., 60945., 67650., 71229., 74439., 72799.,\n",
       "          66742., 62039., 67227., 68661., 66022., 67204., 64519., 51026., 40895.,\n",
       "          33007., 22449., 15423., 10555.,  7312.,  6510.,  6217., 10605., 24691.,\n",
       "          41818., 55367., 60688., 64822., 71262., 72857., 70202., 74147., 79769.,\n",
       "          84099., 83900., 87147., 83617., 75390., 69101., 56408., 38067., 21622.,\n",
       "          13799.,  8834.,  7683.,  7744., 11793., 28572., 44036., 53437., 65542.,\n",
       "          79738., 89153., 89329., 81926., 86289., 89488., 89774., 84194., 83253.,\n",
       "          76138., 59912., 48577., 41804., 27911., 16284.,  9477.,  5627.,  3759.,\n",
       "           3382.,  5243., 13994., 20998., 24532., 29565., 44499., 60791., 65996.,\n",
       "          64179., 68127., 75672., 80841., 83072., 78850., 70501., 63229., 59070.,\n",
       "          53787., 38615., 22150.]),\n",
       "  tensor([])))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "torch.Size([64, 168])\n",
      "torch.Size([1, 64, 168])\n"
     ]
    }
   ],
   "source": [
    "for iter_, sample in enumerate(train_dataloader):\n",
    "    _, _, (input_data, output_data) = sample\n",
    "    print(len(input_data))\n",
    "    print(input_data.shape)\n",
    "    print(input_data.unsqueeze(0).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 혼자 분류된 10번도로만 학습을 잘하는지 돌려보았으나 오차가 너무커서 LSTM은 성능이 안나온다고 판단함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Epoch:  0 | Loss: 71074865152.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  1 | Loss: 71056515072.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  2 | Loss: 71038230528.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  3 | Loss: 71020142592.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  4 | Loss: 71002169344.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  5 | Loss: 70984278016.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  6 | Loss: 70966435840.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  7 | Loss: 70948634624.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  8 | Loss: 70930866176.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch:  9 | Loss: 70913122304.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 10 | Loss: 70895411200.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 11 | Loss: 70877700096.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 12 | Loss: 70860021760.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 13 | Loss: 70842343424.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 14 | Loss: 70824681472.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 15 | Loss: 70807027712.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 16 | Loss: 70789390336.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 17 | Loss: 70771761152.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 18 | Loss: 70754140160.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 19 | Loss: 70736527360.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 20 | Loss: 70718922752.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 21 | Loss: 70701326336.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 22 | Loss: 70683738112.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 23 | Loss: 70666149888.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 24 | Loss: 70648578048.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 25 | Loss: 70631006208.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 26 | Loss: 70613442560.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 27 | Loss: 70595895296.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 28 | Loss: 70578331648.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 29 | Loss: 70560792576.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 30 | Loss: 70543253504.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 31 | Loss: 70525714432.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 32 | Loss: 70508183552.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 33 | Loss: 70490660864.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 34 | Loss: 70473138176.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 35 | Loss: 70455623680.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 36 | Loss: 70438109184.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 37 | Loss: 70420611072.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 38 | Loss: 70403112960.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 39 | Loss: 70385614848.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 40 | Loss: 70368124928.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 41 | Loss: 70350635008.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 42 | Loss: 70333161472.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 43 | Loss: 70315679744.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 44 | Loss: 70298206208.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 45 | Loss: 70280749056.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 46 | Loss: 70263283712.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 47 | Loss: 70245818368.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 48 | Loss: 70228369408.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 49 | Loss: 70210912256.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 50 | Loss: 70193471488.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 51 | Loss: 70176022528.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 52 | Loss: 70158589952.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 53 | Loss: 70141149184.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 54 | Loss: 70123724800.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 55 | Loss: 70106292224.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 56 | Loss: 70088876032.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 57 | Loss: 70071451648.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 58 | Loss: 70054043648.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 59 | Loss: 70036635648.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 60 | Loss: 70019227648.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 61 | Loss: 70001549312.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 62 | Loss: 69984149504.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 63 | Loss: 69966733312.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 64 | Loss: 69949341696.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 65 | Loss: 69931941888.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 66 | Loss: 69914558464.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 67 | Loss: 69897166848.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 68 | Loss: 69879775232.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 69 | Loss: 69862400000.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 70 | Loss: 69845024768.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 71 | Loss: 69827649536.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 72 | Loss: 69810282496.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 73 | Loss: 69792915456.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 74 | Loss: 69775556608.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 75 | Loss: 69758205952.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 76 | Loss: 69740838912.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 77 | Loss: 69723504640.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 78 | Loss: 69706153984.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 79 | Loss: 69688803328.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 80 | Loss: 69671469056.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 81 | Loss: 69653774336.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 82 | Loss: 69636423680.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 83 | Loss: 69619097600.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 84 | Loss: 69601755136.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 85 | Loss: 69584429056.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 86 | Loss: 69567102976.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 87 | Loss: 69549776896.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 88 | Loss: 69532459008.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 89 | Loss: 69515141120.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 90 | Loss: 69497831424.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 91 | Loss: 69480521728.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 92 | Loss: 69463220224.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 93 | Loss: 69445918720.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 94 | Loss: 69428625408.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 95 | Loss: 69411323904.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 96 | Loss: 69394038784.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 97 | Loss: 69376745472.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 98 | Loss: 69359468544.00\n",
      "model saved\n",
      "\n",
      "\n",
      "Valid Epoch: 99 | Loss: 69342191616.00\n",
      "model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_batch_loss = 0.0\n",
    "train_epoch_loss = 0.0\n",
    "\n",
    "valid_epoch_loss = 0.0\n",
    "valid_min_epoch_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()      # 모델을 train mode로 전환. train mode일 때만 적용되어야 하는 drop out 등이 적용될 수 있게 하기 위함 \n",
    "\n",
    "    for iter_, sample in enumerate(train_dataloader):      # enumerate 함수를 통해 train_dataloader에서 'batch의 index'와 'batch'를 순서대로 호출\n",
    "\n",
    "        (h_in, c_in) = (torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device),\n",
    "                        torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device))\n",
    "\n",
    "        _, _, (input_data, output_data) = sample      # train_dataloader에서 불러온 sample은 [[날짜, 시간], [도로], [[input_data],[output_data]]]로 구성됨. 학습에는 [[input_data], [output_data]]만 사용\n",
    "        \n",
    "        input_data = input_data.unsqueeze(0).to(device)\n",
    "        output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "        pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "        \n",
    "        loss = criterion(pred, output_data)\n",
    "\n",
    "        model.zero_grad()    # 파라미터 업데이트는 batch 단위로 이루어지고, 매 batch마다 gradient를 초기화해주어야 함 \n",
    "        loss.backward()      # backpropagation\n",
    "        optimizer.step()      # 파라미터 업데이트\n",
    "        \n",
    "        train_batch_loss += loss.item()\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        if iter_ % 400 == 399:      # 400개의 batch마다 training Loss 출력\n",
    "            print('Train Epoch: {:2} | Batch: {:4} | Loss: {:1.2f}'.format(epoch, iter_+1, train_batch_loss/400))\n",
    "            train_batch_loss = 0\n",
    "            \n",
    "    train_epoch_loss = 0.0\n",
    "\n",
    "    \n",
    "    model.eval()      # 모델을 eval mode로 전환. eval mode에서 적용되면 안되는 drop out 등이 적용되지 않게 하기 위함\n",
    "\n",
    "    with torch.no_grad():      # validation / test set에 대해서는 weight 및 bias의 update, 즉, gradient descent가 일어나지 않도록 no_grad()를 선언\n",
    "        (h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                        torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "        for iter_, sample in enumerate(validate_dataloader):      # enumerate 함수를 통해 validate_dataloader에서 'batch의 index'와 'batch'를 순서대로 호출\n",
    "\n",
    "            _, _, (input_data, output_data) = sample      # validate_dataloader에서 불러온 sample은 [[날짜, 시간], [도로], [[input_data],[output_data]]]로 구성됨. validation에는 [[input_data], [output_data]]만 사용\n",
    "\n",
    "            input_data = input_data.unsqueeze(0).to(device)\n",
    "            output_data = output_data.unsqueeze(0).to(device)\n",
    "\n",
    "            pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "            loss = criterion(pred, output_data)\n",
    "            valid_epoch_loss += loss.item()\n",
    "\n",
    "        print('\\nValid Epoch: {:2} | Loss: {:1.2f}'.format(epoch, valid_epoch_loss/len(validate_dataloader)))\n",
    "\n",
    "        if valid_epoch_loss < valid_min_epoch_loss:\n",
    "            save_model('best', model, optimizer)\n",
    "            valid_min_epoch_loss = valid_epoch_loss\n",
    "\n",
    "        valid_epoch_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "seq_len = 7\n",
    "\n",
    "input_size = seq_len * 24\n",
    "hidden_size = 1024\n",
    "output_size = input_size\n",
    "batch_size = 1\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = data_loader(root=DATASET_PATH,\n",
    "                              phase='test',\n",
    "                              batch_size=batch_size,\n",
    "                              seq_len=seq_len,\n",
    "                              drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "# model\n",
    "model_name = 'log/best.pth'\n",
    "\n",
    "load_model(model_name, model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\n",
    "submission_table = pd.read_csv(submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "for iter_, sample in enumerate(test_dataloader):\n",
    "\n",
    "    timestamp, category, (input_data, output_data) = sample\n",
    "    input_data = input_data.unsqueeze(0).to(device)\n",
    "\n",
    "    pred, (h_in, c_in) = model(input_data, h_in, c_in)\n",
    "\n",
    "    for i, (t, h) in enumerate(zip(timestamp[0], timestamp[1])):\n",
    "        for cat, row in zip(category, pred[0]):\n",
    "            cat = f'{cat}'\n",
    "            submission_table[cat] = row.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_table.to_csv('prediction_drop.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2337.,  1810.,  1530.,  1882.,  3656.,  9440., 19625., 27199.,\n",
       "          24872., 21953., 22573., 22182., 19528., 19447., 20903., 21353.,\n",
       "          22821., 24493., 22935., 15875., 10581.,  7361.,  5305.,  3745.,\n",
       "           2476.,  1667.,  1703.,  1744.,  3588.,  7436., 15250., 24448.,\n",
       "          23720., 22650., 23055., 22253., 19605., 19215., 21583., 22555.,\n",
       "          23532., 25202., 23263., 14596., 10501.,  7269.,  5204.,  3758.,\n",
       "           2233.,  1691.,  1384.,  1928.,  3415.,  8062., 15565., 24127.,\n",
       "          24285., 22211., 23793., 22991., 19905., 19955., 21720., 22275.,\n",
       "          23603., 24844., 16339., 15929., 10519.,  7940.,  5640.,  3769.,\n",
       "           2622.,  1511.,  1644.,  1789.,  2946.,  7822., 15242., 23926.,\n",
       "          23276., 22049., 23750., 21962., 19574., 20372., 21838., 22048.,\n",
       "          24353., 26552., 25576., 16449., 12398.,  8502.,  6246.,  4312.,\n",
       "           2368.,  1696.,  1590.,  1866.,  2673.,     0., 12483., 20530.,\n",
       "          21995., 19800., 21407., 20913., 19318., 19993., 21720., 24012.,\n",
       "          25872., 28256., 28208., 20315., 13167.,  9074.,  6046.,  3681.,\n",
       "           2489.,  2022.,  1585.,  1514.,  2615.,  5284.,  9877., 12866.,\n",
       "          15563., 20606., 26031., 27478., 27085., 26984., 29522., 29537.,\n",
       "          29342., 28899., 24235., 17198., 13085., 11246.,  8282.,  4698.,\n",
       "           2699.,  2208.,  1576.,  1380.,  1691.,  3574.,  6338.,  7408.,\n",
       "           9959., 14959., 20335., 24907., 23863., 24869., 27925., 29273.,\n",
       "          30145., 27940., 25551., 20194., 17962., 14031.,  8889.,  4606.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
